{"input_text": "Create a DataFrame with ByteType column named 'byte_col'.","output_text": "spark.createDataFrame([(1,), (2,), (3,)], ['byte_col'], ByteType())"},
{"input_text": "Create a DataFrame with ShortType column named 'short_col'.", "output_text": "spark.createDataFrame([(100,), (-200,), (300,), (-400,)], ['short_col'], ShortType())"},
{"input_text": "Create a DataFrame with IntegerType column named 'int_col'.","output_text": "spark.createDataFrame([(1000,), (-2000,), (3000,), (-4000,)], ['int_col'], IntegerType())"},
{"input_text": "Create a DataFrame with LongType column named 'long_col'.","output_text": "spark.createDataFrame([(10000000000,), (-20000000000,), (30000000000,), (-40000000000,)], ['long_col'], LongType())"},
{"input_text": "Create a DataFrame with FloatType column named 'float_col'.","output_text": "spark.createDataFrame([(1.1,), (-2.2,), (3.3,), (-4.4,)], ['float_col'], FloatType())"},
{"input_text": "Create a DataFrame with DoubleType column named 'double_col'.","output_text": "spark.createDataFrame([(1.1,), (-2.2,), (3.3,), (-4.4,)], ['double_col'], DoubleType())"},
{"input_text": "Create a DataFrame with DecimalType column named 'decimal_col'.","output_text": "spark.createDataFrame([(Decimal('123.45'),), (Decimal('-678.90'),)], ['decimal_col'], DecimalType())"},
{"input_text": "Create a DataFrame with StringType column named 'string_col'.","output_text": "spark.createDataFrame([('foo',), ('bar',), ('baz',)], ['string_col'], StringType())"},
{"input_text": "Create a DataFrame with BinaryType column named 'binary_col'.", "output_text": "spark.createDataFrame([(bytearray(b'hello'),), (bytearray(b'world'),)], ['binary_col'], BinaryType())"},
{"input_text": "Create a DataFrame with BooleanType column named 'bool_col'.", "output_text": "spark.createDataFrame([(True,), (False,), (True,)], ['bool_col'], BooleanType())"}
{"input_text": "Create a DataFrame with TimestampType column named 'timestamp_col'.", "output_text": "spark.createDataFrame([(datetime.datetime(2022, 1, 1, 12, 0),), (datetime.datetime(2022, 1, 2, 12, 0),)], ['timestamp_col'], TimestampType())"},
{"input_text": "Create a DataFrame with DateType column named 'date_col'.", "output_text": "spark.createDataFrame([(datetime.date(2022, 1, 1),), (datetime.date(2022, 1, 2),)], ['date_col'], DateType())"},
{"input_text": "Create a DataFrame with DayTimeIntervalType column named 'day_time_interval_col'.", "output_text": "spark.createDataFrame([(datetime.timedelta(days=1, hours=2, minutes=30),), (datetime.timedelta(days=0, hours=8, minutes=45),)], ['day_time_interval_col'], DayTimeIntervalType())"},
{"input_text": "Create a DataFrame with ArrayType column named 'array_col'.", "output_text": "spark.createDataFrame([([1, 2, 3],), ([4, 5, 6],)], ['array_col'], ArrayType(IntegerType()))"},
{"input_text": "Create a DataFrame with MapType column named 'map_col'.", "output_text": "spark.createDataFrame([({'a': 1, 'b': 2},), ({'x': 10, 'y': 20},)], ['map_col'], MapType(StringType(), IntegerType()))"},
{"input_text": "Create a DataFrame with StructType column named 'struct_col'.", "output_text": "spark.createDataFrame([((1, 'a'),), ((2, 'b'),)], ['struct_col'], StructType([StructField('int_field', IntegerType()), StructField('str_field', StringType())]))"}
{"input_text": "Create a DataFrame with BinaryType column named 'binary_col_2'.", "output_text": "spark.createDataFrame([(bytearray(b'hello'),), (bytearray(b'world'),)], ['binary_col_2'], BinaryType())"},
{"input_text": "Create a DataFrame with BooleanType column named 'bool_col_2'.", "output_text": "spark.createDataFrame([(True,), (False,), (True,)], ['bool_col_2'], BooleanType())"},
{"input_text": "Create a DataFrame with TimestampType column named 'timestamp_col_2'.", "output_text": "spark.createDataFrame([(datetime.datetime(2022, 1, 1, 12, 0),), (datetime.datetime(2022, 1, 2, 12, 0),)], ['timestamp_col_2'], TimestampType())"},
{"input_text": "Create a DataFrame with DateType column named 'date_col_2'.", "output_text": "spark.createDataFrame([(datetime.date(2022, 1, 1),), (datetime.date(2022, 1, 2),)], ['date_col_2'], DateType())"},
{"input_text": "Create a DataFrame with StringType column named 'string_col_2'.", "output_text": "spark.createDataFrame([('foo',), ('bar',), ('baz',)], ['string_col_2'], StringType())"},
{"input_text": "Create a DataFrame with ArrayType column named 'array_col_5'.", "output_text": "spark.createDataFrame([([[1, 2, 3], [4, 5, 6]],), ([[7, 8, 9], [10, 11, 12]],)], ['array_col_5'], ArrayType(ArrayType(IntegerType())))"},
{"input_text": "Create a DataFrame with MapType column named 'map_col_5'.", "output_text": "spark.createDataFrame([({'a': {'x': 1, 'y': 2}, 'b': {'x': 3, 'y': 4}},), ({'x': {'a': 5, 'b': 6}, 'y': {'a': 7, 'b': 8}},)], ['map_col_5'], MapType(StringType(), MapType(StringType(), IntegerType())))"},
{"input_text": "Create a DataFrame with StructType column named 'struct_col_5'.", "output_text": "spark.createDataFrame([(((1, 'a'), {'x': 2, 'y': 3}),), (((4, 'b'), {'x': 5, 'y': 6}),)], ['struct_col_5'], StructType([StructField('struct_field_1', StructType([StructField('int_field', IntegerType()), StructField('str_field', StringType())])), StructField('struct_field_2', MapType(StringType(), IntegerType()))]))"},
{"input_text": "Create a DataFrame with ArrayType column named 'array_col_6'.", "output_text": "spark.createDataFrame([(['foo', 'bar', 'baz'],), (['abc', 'def', 'ghi'],)], ['array_col_6'], ArrayType(StringType()))"},
{"input_text": "Create a DataFrame with MapType column named 'map_col_6'.", "output_text": "spark.createDataFrame([({'a': 'foo', 'b': 'bar'},), ({'x': 'abc', 'y': 'def'},)], ['map_col_6'], MapType(StringType(), StringType()))"},
{"input_text": "Create a DataFrame with StructType column named 'struct_col_6'.", "output_text": "spark.createDataFrame([(('foo', 123),), (('bar', 456),)], ['struct_col_6'], StructType([StructField('str_field', StringType()), StructField('int_field', IntegerType())]))"},
{"input_text": "Create a DataFrame with ArrayType column named 'array_col_7'.", "output_text": "spark.createDataFrame([([(1, 'a'), (2, 'b'), (3, 'c')],), ([(4, 'x'), (5, 'y'), (6, 'z')],)], ['array_col_7'], ArrayType(StructType([StructField('int_field', IntegerType()), StructField('str_field', StringType())])))"},
{"input_text": "Create a DataFrame with MapType column named 'map_col_7'.", "output_text": "spark.createDataFrame([({'a': (1, 'foo'), 'b': (2, 'bar')},), ({'x': (10, 'abc'), 'y': (20, 'def')},)], ['map_col_7'], MapType(StringType(), StructType([StructField('int_field', IntegerType()), StructField('str_field', StringType())])))"},
{"input_text": "Create a DataFrame with StructType column named 'struct_col_7'.", "output_text": "spark.createDataFrame([(((1, 'a'), (2, 'b')),), (((3, 'x'), (4, 'y')),)], ['struct_col_7'], StructType([StructField('struct_field_1', StructType([StructField('int_field', IntegerType()), StructField('str_field', StringType())])), StructField('struct_field_2', StructType([StructField('int_field', IntegerType()), StructField('str_field', StringType())]))]))"}
{"input_text": "Update values in 'int_col' column of DataFrame 'df' where 'string_col' equals 'foo'.", "output_text": "df = df.withColumn('int_col', when(col('string_col') == 'foo', 999).otherwise(col('int_col')))"},
{"input_text": "Delete rows from DataFrame 'df' where 'bool_col' is False.", "output_text": "df = df.filter(col('bool_col') != False)"},
{"input_text": "Add a new column named 'new_col' to DataFrame 'df' with default value 123.", "output_text": "df = df.withColumn('new_col', lit(123))"},
{"input_text": "Update values in 'array_col' column of DataFrame 'df' where 'string_col' equals 'foo'.", "output_text": "df = df.withColumn('array_col', when(col('string_col') == 'foo', array(lit('updated_value'))).otherwise(col('array_col')))"},
{"input_text": "Drop column named 'drop_col' from DataFrame 'df'.", "output_text": "df = df.drop('drop_col')"},
{"input_text": "Rename column 'old_col' to 'new_col' in DataFrame 'df'.", "output_text": "df = df.withColumnRenamed('old_col', 'new_col')"},
{"input_text": "Update values in 'map_col' column of DataFrame 'df' where 'string_col' equals 'foo'.", "output_text": "df = df.withColumn('map_col', when(col('string_col') == 'foo', create_map(lit('key'), lit('updated_value'))).otherwise(col('map_col')))"},
{"input_text": "Add a new column named 'new_struct_col' to DataFrame 'df' with default value of a struct containing integer and string fields.", "output_text": "df = df.withColumn('new_struct_col', struct(lit(123), lit('abc')))"},
{"input_text": "Drop rows from DataFrame 'df' where 'int_col' is less than 0.", "output_text": "df = df.filter(col('int_col') >= 0)"},
{"input_text": "Update elements in 'array_col' column of DataFrame 'df' where 'string_col' equals 'foo'.", "output_text": "df = df.withColumn('array_col', when(col('string_col') == 'foo', array_remove(col('array_col'), lit('element_to_remove'))).otherwise(col('array_col')))"},
{"input_text": "Add a new column named 'new_map_col' to DataFrame 'df' with default value of a map containing string keys and integer values.", "output_text": "df = df.withColumn('new_map_col', create_map(lit('key1'), lit(123), lit('key2'), lit(456)))"}
{"input_text": "Delete rows from DataFrame 'df' where 'date_col' is before '2022-01-01'.", "output_text": "df = df.filter(col('date_col') >= lit('2022-01-01'))"},
{"input_text": "Add a new column named 'new_array_col' to DataFrame 'df' with default value of an array containing integer values.", "output_text": "df = df.withColumn('new_array_col', array(lit(1), lit(2), lit(3)))"},
{"input_text": "Drop rows from DataFrame 'df' where 'timestamp_col' is null.", "output_text": "df = df.filter(col('timestamp_col').isNotNull())"},
{"input_text": "Update elements in 'map_col' column of DataFrame 'df' where 'string_col' equals 'foo'.", "output_text": "df = df.withColumn('map_col', when(col('string_col') == 'foo', map_concat(col('map_col'), create_map(lit('new_key'), lit('new_value')))).otherwise(col('map_col')))"},
{"input_text": "Add a new column named 'new_struct_col' to DataFrame 'df' with default value of a struct containing string and boolean fields.", "output_text": "df = df.withColumn('new_struct_col', struct(lit('abc'), lit(True)))"},
{"input_text": "Drop rows from DataFrame 'df' where 'binary_col' is empty.", "output_text": "df = df.filter(size(col('binary_col')) > 0)"},
{"input_text": "Update values in 'int_col' column of DataFrame 'df' where 'string_col' starts with 'prefix'.", "output_text": "df = df.withColumn('int_col', when(col('string_col').startswith('prefix'), 999).otherwise(col('int_col')))"},
{"input_text": "Add a new column named 'new_map_col' to DataFrame 'df' with default value of a map containing string keys and array values.", "output_text": "df = df.withColumn('new_map_col', create_map(lit('key1'), array(lit(1), lit(2)), lit('key2'), array(lit(3), lit(4))))"},
{"input_text": "Drop rows from DataFrame 'df' where 'bool_col' is True.", "output_text": "df = df.filter(col('bool_col') != True)"},
{"input_text": "Update elements in 'array_col' column of DataFrame 'df' where 'string_col' equals 'foo'.", "output_text": "df = df.withColumn('array_col', when(col('string_col') == 'foo', array_remove(col('array_col'), lit('element_to_remove'))).otherwise(col('array_col')))"},
{"input_text": "Add a new column named 'new_struct_col' to DataFrame 'df' with default value of a struct containing float and boolean fields.", "output_text": "df = df.withColumn('new_struct_col', struct(lit(1.23), lit(True)))"}
{"input_text": "Update values in 'double_col' column of DataFrame 'df' where 'string_col' ends with 'suffix'.", "output_text": "df = df.withColumn('double_col', when(col('string_col').endswith('suffix'), 999.99).otherwise(col('double_col')))"},
{"input_text": "Delete rows from DataFrame 'df' where 'long_col' is greater than 1000.", "output_text": "df = df.filter(col('long_col') <= 1000)"},
{"input_text": "Add a new column named 'new_binary_col' to DataFrame 'df' with default value of a binary array.", "output_text": "df = df.withColumn('new_binary_col', lit(bytearray(b'hello')))"},
{"input_text": "Update values in 'float_col' column of DataFrame 'df' where 'string_col' contains 'substring'.", "output_text": "df = df.withColumn('float_col', when(col('string_col').contains('substring'), 123.45).otherwise(col('float_col')))"},
{"input_text": "Add a new column named 'new_date_col' to DataFrame 'df' with default value of a date.", "output_text": "df = df.withColumn('new_date_col', lit(datetime.date(2022, 1, 1)))"},
{"input_text": "Drop rows from DataFrame 'df' where 'timestamp_col' is after '2022-12-31'.", "output_text": "df = df.filter(col('timestamp_col') <= lit('2022-12-31'))"},
{"input_text": "Update values in 'decimal_col' column of DataFrame 'df' where 'string_col' matches 'pattern'.", "output_text": "df = df.withColumn('decimal_col', when(col('string_col').rlike('pattern'), Decimal('123.456')).otherwise(col('decimal_col')))"},
{"input_text": "Add a new column named 'new_boolean_col' to DataFrame 'df' with default value of a boolean.", "output_text": "df = df.withColumn('new_boolean_col', lit(True))"},
{"input_text": "Drop rows from DataFrame 'df' where 'date_col' is null.", "output_text": "df = df.filter(col('date_col').isNotNull())"},
{"input_text": "Update values in 'string_col' column of DataFrame 'df' where 'bool_col' is True.", "output_text": "df = df.withColumn('string_col', when(col('bool_col') == True, 'updated_value').otherwise(col('string_col')))"},
{"input_text": "Add a new column named 'new_timestamp_col' to DataFrame 'df' with default value of a timestamp.", "output_text": "df = df.withColumn('new_timestamp_col', lit(datetime.datetime(2022, 1, 1, 12, 0)))"},
{"input_text": "Drop rows from DataFrame 'df' where 'binary_col' is null.", "output_text": "df = df.filter(col('binary_col').isNotNull())"}
{"input_text": "Update values in 'int_col' column of DataFrame 'df' where 'string_col' matches the regex pattern.", "output_text": "df = df.withColumn('int_col', when(col('string_col').rlike('regex_pattern'), 999).otherwise(col('int_col')))"},
{"input_text": "Delete rows from DataFrame 'df' where 'long_col' is less than -1000.", "output_text": "df = df.filter(col('long_col') >= -1000)"},
{"input_text": "Add a new column named 'new_string_col' to DataFrame 'df' with default value of a string.", "output_text": "df = df.withColumn('new_string_col', lit('default_value'))"},
{"input_text": "Update values in 'float_col' column of DataFrame 'df' where 'string_col' is in a list of values.", "output_text": "df = df.withColumn('float_col', when(col('string_col').isin(['value1', 'value2']), 123.45).otherwise(col('float_col')))"},
{"input_text": "Add a new column named 'new_double_col' to DataFrame 'df' with default value of a double.", "output_text": "df = df.withColumn('new_double_col', lit(123.45))"},
{"input_text": "Drop rows from DataFrame 'df' where 'timestamp_col' is within a specific range.", "output_text": "df = df.filter((col('timestamp_col') >= lit('start_timestamp')) & (col('timestamp_col') <= lit('end_timestamp')))"},
{"input_text": "Update values in 'decimal_col' column of DataFrame 'df' where 'string_col' is not null.", "output_text": "df = df.withColumn('decimal_col', when(col('string_col').isNotNull(), Decimal('123.456')).otherwise(col('decimal_col')))"},
{"input_text": "Add a new column named 'new_boolean_col' to DataFrame 'df' with default value of a boolean array.", "output_text": "df = df.withColumn('new_boolean_col', array(lit(True), lit(False)))"},
{"input_text": "Drop rows from DataFrame 'df' where 'date_col' is outside a specific date range.", "output_text": "df = df.filter((col('date_col') >= lit('start_date')) & (col('date_col') <= lit('end_date')))"},
{"input_text": "Update values in 'string_col' column of DataFrame 'df' where 'bool_col' is False.", "output_text": "df = df.withColumn('string_col', when(col('bool_col') == False, 'updated_value').otherwise(col('string_col')))"},
{"input_text": "Add a new column named 'new_timestamp_col' to DataFrame 'df' with default value of a timestamp array.", "output_text": "df = df.withColumn('new_timestamp_col', array(lit(datetime.datetime(2022, 1, 1, 12, 0)), lit(datetime.datetime(2022, 1, 2, 12, 0))))"},
{"input_text": "Drop rows from DataFrame 'df' where 'binary_col' contains specific values.", "output_text": "df = df.filter(~col('binary_col').isin([bytearray(b'value1'), bytearray(b'value2')]))"}
{"input_text": "Perform a group-by operation on DataFrame 'df' to calculate the average value of 'numeric_col' for each unique value in 'category_col', then order the results by average value in descending order, and select the top 10 rows.", "output_text": "result_df = df.groupBy('category_col').agg(avg('numeric_col').alias('avg_numeric')).orderBy(col('avg_numeric').desc()).limit(10)"}
{"input_text": "Aggregate DataFrame 'df' by month and product, calculating the average sales for each combination, for the last year's data.", "output_text": "result_df = df.filter(year('date_col') == year(current_date()) - 1).groupBy(month('date_col').alias('month'), 'product').agg(avg('sales').alias('avg_sales'))"}
{"input_text": "Calculate the correlation between 'numeric_col1' and 'numeric_col2' in DataFrame 'df'.", "output_text": "correlation = df.stat.corr('numeric_col1', 'numeric_col2')"}
{"input_text": "Calculate the z-score for each value in 'numeric_col' of DataFrame 'df', and flag values with z-score greater than 3 or less than -3 as outliers.", "output_text": "z_score_df = df.withColumn('z_score', (col('numeric_col') - avg('numeric_col').over(Window.partitionBy())).alias('mean')).select('numeric_col', 'z_score').filter(abs(col('z_score')) > 3)"}
{"input_text": "Apply k-means clustering algorithm on DataFrame 'df' using features such as 'feature1' and 'feature2' to segment customers into 5 clusters.", "output_text": "model = KMeans(k=5, featuresCol='features', predictionCol='cluster').fit(df)"}
{"input_text": "Perform cohort analysis on DataFrame 'df' to calculate the retention rate of users by cohort over multiple months.", "output_text": "retention_df = df.withColumn('cohort_month', trunc('signup_date', 'MM')).groupBy('cohort_month', 'month_diff').agg(countDistinct('user_id').alias('users')).withColumn('retention_rate', col('users') / first('users').over(Window.partitionBy('cohort_month')))"}
{"input_text": "Calculate a 3-month moving average of sales in DataFrame 'df' to identify trends over time.", "output_text": "result_df = df.withColumn('moving_avg', avg('sales').over(Window.orderBy('date_col').rowsBetween(-2, 0)))"}
{"input_text": "Perform market basket analysis on DataFrame 'df' to find frequently co-occurring items using the Apriori algorithm.", "output_text": "association_rules = FPGrowth(itemsCol='items', minSupport=0.1, minConfidence=0.5).fit(df).associationRules"}
{"input_text": "Estimate the Customer Lifetime Value (CLV) for each customer in DataFrame 'df' using historical transaction data.", "output_text": "clv_df = df.groupBy('customer_id').agg(sum('purchase_value').alias('total_purchase'), datediff(max('purchase_date'), min('purchase_date')).alias('customer_age')).withColumn('clv', col('total_purchase') * 0.1 / col('customer_age'))"}
{"input_text": "Perform geospatial analysis on DataFrame 'df' to visualize the distribution of customers across different regions using latitude and longitude data.", "output_text": "geo_df = df.withColumn('geohash', geohash_udf('latitude', 'longitude')).groupBy('geohash').agg(count('customer_id').alias('customer_count'))"}
{"input_text": "Perform sentiment analysis on DataFrame 'df' containing customer reviews to determine sentiment polarity using the VADER sentiment analysis tool.", "output_text": "sentiment_df = df.withColumn('sentiment', vader_udf('review_text'))"}
{"input_text": "Train a machine learning model on DataFrame 'df' to predict customer churn based on features such as 'feature1', 'feature2', etc.", "output_text": "model = RandomForestClassifier(featuresCol='features', labelCol='churn', numTrees=100).fit(df)"}
{"input_text": "Perform time decay analysis on DataFrame 'df' to assign weights to historical data based on recency, with recent data having higher weights.", "output_text": "weighted_df = df.withColumn('weight', exp((datediff(current_date(), 'transaction_date') * -0.01)))"}
{"input_text": "Perform RFM analysis on DataFrame 'df' to segment customers into different groups based on their recency, frequency, and monetary values.", "output_text": "rfm_df = df.groupBy('customer_id').agg(max('transaction_date').alias('recency'), count('transaction_id').alias('frequency'), sum('transaction_value').alias('monetary'))"}
{"input_text": "Perform feature engineering on DataFrame 'df' to create new features such as 'feature1_squared' (square of 'feature1') and 'feature2_log' (logarithm of 'feature2').", "output_text": "feature_engineered_df = df.withColumn('feature1_squared', col('feature1') ** 2).withColumn('feature2_log', log(col('feature2')))"} 
{"input_text": "Apply hierarchical clustering algorithm on DataFrame 'df' to group similar items into hierarchical clusters.", "output_text": "clusters = AgglomerativeClustering(n_clusters=5, linkage='ward').fit(df)"}
{"input_text": "Train a time series forecasting model on DataFrame 'df' to predict future sales using historical sales data.", "output_text": "forecast_model = ARIMA(df['sales'], order=(5,1,0)).fit()" }
{"input_text": "Perform network analysis on DataFrame 'df' representing a network graph to identify key entities and their connections.", "output_text": "network_analysis_results = nx.algorithms.centrality.betweenness_centrality(df)"}
{"input_text": "Train a predictive maintenance model on DataFrame 'df' to predict equipment failures using sensor data and historical failure records.", "output_text": "maintenance_model = RandomForestClassifier(featuresCol='sensor_data', labelCol='failure', numTrees=100).fit(df)"}
{"input_text": "Perform text mining on DataFrame 'df' containing text documents to extract key phrases using natural language processing techniques.", "output_text": "key_phrases = df.select('document_id', extract_key_phrases_udf('text').alias('key_phrases'))"}
{"input_text": "Implement collaborative filtering on DataFrame 'df' to recommend items to users based on their interactions.", "output_text": "recommendation_model = ALS(rank=10, maxIter=10, regParam=0.01, userCol='user_id', itemCol='item_id', ratingCol='rating').fit(df)"}
{"input_text": "Perform survival analysis on DataFrame 'df' to estimate the time until failure for each unit using Cox proportional hazards model.", "output_text": "survival_analysis_results = lifelines.CoxPHFitter().fit(df, duration_col='time_to_failure', event_col='failure_event')"}
{"input_text": "Apply SMOTE (Synthetic Minority Over-sampling Technique) on DataFrame 'df' to balance the class distribution in a classification task.", "output_text": "balanced_df = SMOTE().fit_resample(df.drop('target', axis=1), df['target'])"}
{"input_text": "Perform dimensionality reduction on DataFrame 'df' using PCA (Principal Component Analysis) to project data onto a lower-dimensional space.", "output_text": "reduced_df = PCA(n_components=2).fit_transform(df)"}
{"input_text": "Perform time window analysis on DataFrame 'df' to calculate aggregated statistics within each hour window.", "output_text": "windowed_df = df.groupBy(window('timestamp_col', '1 hour')).agg(avg('value').alias('avg_value'), max('value').alias('max_value'))"}
{"input_text": "Apply frequent itemset mining on DataFrame 'df' to find itemsets that frequently co-occur in transactions.", "output_text": "frequent_itemsets = fpgrowth(df, min_support=0.1, min_confidence=0.5)"}
{"input_text": "Perform ANOVA analysis on DataFrame 'df' to compare means across multiple groups using 'numeric_col' as the dependent variable and 'category_col' as the independent variable.", "output_text": "anova_results = stats.f_oneway(*[group['numeric_col'] for name, group in df.groupby('category_col')])"}
{"input_text": "Conduct spatial analysis on DataFrame 'df' to identify spatial clusters using DBSCAN (Density-Based Spatial Clustering of Applications with Noise).", "output_text": "spatial_clusters = DBSCAN(eps=0.1, min_samples=5).fit(df[['latitude', 'longitude']])"}
{"input_text": "Perform model evaluation on DataFrame 'df' using cross-validation to assess the performance of a classifier.", "output_text": "evaluation_metrics = cross_val_score(classifier, df.drop('target', axis=1), df['target'], cv=5, scoring='accuracy')"}
{
    "input_text": "Rank employees within each department based on their salary.",
    "output_text": "val windowSpec = Window.partitionBy('department').orderBy('salary')
                    val rankedEmployees = df.withColumn('rank', rank().over(windowSpec))"
  }
  {
    "input_text": "Extract specific fields from nested JSON data.",
    "output_text": "val extractedData = df.select('field1', 'field2.nested_field1', 'field2.nested_field2')"
  }
  {
    "input_text": "Filter data for a specific date range.",
    "output_text": "val filteredData = df.filter(col('date_column').between('2022-01-01', '2022-12-31'))"
  }
  {
    "input_text": "Compute the 7-day moving average of stock prices for each stock.",
    "output_text": "val windowSpec = Window.partitionBy('stock_symbol').orderBy('trade_date').rowsBetween(-6, 0)
                    val movingAvg = df.withColumn('moving_avg', avg('stock_price').over(windowSpec))"
  }
  {
    "input_text": "Fill missing values in a column with the mean value.",
    "output_text": "val imputedData = df.withColumn('filled_column', when(col('column').isNull, mean('column').over(Window.partitionBy())).otherwise(col('column')))"
  }
  {
    "input_text": "Pivot data to transform rows into columns based on a categorical column.",
    "output_text": "val pivotedData = df.groupBy('category_column').pivot('pivot_column').agg(sum('value_column'))"
  }
  {
    "input_text": "Extract fields from a struct within a DataFrame.",
    "output_text": "val extractedFields = df.select('struct_column.field1', 'struct_column.field2')"
  }
  {
    "input_text": "Tokenize text data and perform word count.",
    "output_text": "val tokenizedData = df.withColumn('words', split('text_column', ' ')).select(explode('words').alias('word')).groupBy('word').count()"
  }
  {
    "input_text": "Join two DataFrames based on a common key.",
    "output_text": "val joinedData = df1.join(df2, df1('common_key') === df2('common_key'))"
  }
  {
    "input_text": "Explode nested arrays into separate rows.",
    "output_text": "val explodedData = df.withColumn('exploded_array', explode('nested_array_column'))"
  }
  {
    "input_text": "Aggregate streaming data over a sliding window of 1 hour.",
    "output_text": "val windowedAggregation = streamingDF
                      .groupBy(window($'timestamp', '1 hour'))
                      .agg(count('*').alias('count'))"
  }
  {
    "input_text": "Apply a user-defined function (UDF) to extract information from a complex data type.",
    "output_text": "val extractInfoUDF = udf((structColumn: StructType) => structColumn.field, StringType)
                    val extractedData = df.withColumn('extracted_field', extractInfoUDF('complex_column'))"
  }
  {
    "input_text": "Filter data based on a regular expression pattern.",
    "output_text": "val filteredData = df.filter(col('text_column').rlike('pattern'))"
  }
  {
    "input_text": "Convert timestamps from one time zone to another.",
    "output_text": "val convertedData = df.withColumn('converted_timestamp', from_utc_timestamp('timestamp_column', 'America/New_York'))"
  }
  {
    "input_text": "Union batch and streaming DataFrames into a single DataFrame.",
    "output_text": "val unionedData = batchDF.union(streamingDF)"
  }
  {
    "input_text": "Explode nested arrays and perform aggregation.",
    "output_text": "val explodedData = df.withColumn('exploded_column', explode('nested_array'))
                    val aggregatedData = explodedData.groupBy('id').agg(sum('exploded_column'))"
  }
  {
    "input_text": "Analyze temporal data by comparing current and previous values.",
    "output_text": "val laggedData = df.withColumn('lag_value', lag('value', 1).over(Window.orderBy('timestamp')))
                    val leadData = df.withColumn('lead_value', lead('value', 1).over(Window.orderBy('timestamp')))"
  }
  {
    "input_text": "Extract specific fields from JSON data using Spark SQL.",
    "output_text": "val extractedData = spark.sql('SELECT field1, field2.nested_field FROM json_table')"
  }
  {
    "input_text": "Perform windowed aggregation with custom range.",
    "output_text": "val windowSpec = Window.partitionBy('category').orderBy('timestamp').rangeBetween(-7, 0)
                    val aggregatedData = df.withColumn('rolling_avg', avg('value').over(windowSpec))"
  }
  {
    "input_text": "Perform join operation with broadcast and bucketing optimization.",
    "output_text": "val broadcastedTable = spark.sparkContext.broadcast(broadcastDF)
                    val joinedData = bucketedDF.join(broadcastedTable.value, bucketedDF('key') === broadcastedTable('key'))"
  }
  {
    "input_text": "Handle JSON data with evolving schema using DataFrame API.",
    "output_text": "val jsonDF = spark.read.json('s3://path/to/json_data')
                    jsonDF.printSchema()"
  }
  {
    "input_text": "Analyze temporal data by bucketing into hourly intervals and calculating aggregate statistics.",
    "output_text": "val bucketedData = df.groupBy(window('timestamp', '1 hour')).agg(sum('value'), avg('value'))"
  }
  {
    "input_text": "Perform stateful aggregation on streaming data to calculate cumulative sums.",
    "output_text": "val statefulAggregation = streamingDF.groupByKey(_.key).mapGroupsWithState(StateSpec.function(mappingFunction))"
  }
  {
    "input_text": "Perform spatial analysis on geospatial data using GeoSpark library.",
    "output_text": "val spatialAnalysis = df.select(ST_Intersects('geometry_column', 'geometry_polygon'))"
  }
  {
    "input_text": "Perform text analysis by transforming text data into TF-IDF vectors.",
    "output_text": "val tfidfVectorizer = new HashingTF().transform(tokenizedData)
                    val idfModel = new IDF().fit(tfidfVectorizer)"
  }
  {
    "input_text": "Read Avro data from Kafka using Confluent Schema Registry.",
    "output_text": "val avroDF = spark.read.format('avro').load('kafka://path/to/avro_data')"
  }
  {
    "input_text": "Analyze temporal data by comparing current and previous values using lag and lead functions.",
    "output_text": "val laggedData = df.withColumn('lag_value', lag('value', 1).over(Window.orderBy('timestamp')))
                    val leadData = df.withColumn('lead_value', lead('value', 1).over(Window.orderBy('timestamp')))"
  }
  {
    "input_text": "Flatten nested JSON data to simplify schema and improve query performance.",
    "output_text": "val flattenedData = df.select(flatten(struct('nested_struct.*')))"
  }
  {
    "input_text": "Perform graph analysis on connected data using GraphFrames library.",
    "output_text": "val graph = GraphFrame(verticesDF, edgesDF)
                    val shortestPaths = graph.shortestPaths.landmarks(Seq('source', 'target'))"
  }
  {
    "input_text": "Impute missing values in DataFrame columns using mean or median imputation.",
    "output_text": "val imputedDF = df.na.fill(meanValue, Seq('column1', 'column2'))"
  }
  {
    "input_text": "Perform complex operations on nested arrays using Spark SQL higher-order functions.",
    "output_text": "val processedData = df.withColumn('processed_array', transform(col('nested_array'), element => expr))"
  }
  {
    "input_text": "Perform geospatial analysis and raster processing using GeoTrellis library.",
    "output_text": "val raster = Raster('path_to_raster_file')
                    val result = raster.tileToLayout(layoutDefinition)"
  }
  {
    "input_text": "Aggregate streaming data with watermarking to handle late arrivals.",
    "output_text": "val aggregatedStream = streamingDF.withWatermark('timestamp', '1 hour').groupBy(window('timestamp', '1 hour')).agg(sum('value'))"
  }
  {
    "input_text": "Perform complex data transformations using Spark SQL functions.",
    "output_text": "val transformedData = df.withColumn('new_column', expr('complex_function(column)'))"
  }
  {
    "input_text": "Build and evaluate machine learning models using MLlib library.",
    "output_text": "val model = RandomForest.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)
                    val predictions = model.predict(testData)"
  }
  {
    "input_text": "Perform time series analysis by applying windowed functions and temporal operations.",
    "output_text": "val timeSeriesAnalysis = df.withColumn('rolling_avg', avg('value').over(Window.partitionBy('category').orderBy('timestamp').rowsBetween(-3, 0)))"
  }
  {
    "input_text": "Read and process XML data using Databricks Spark-XML library.",
    "output_text": "val xmlDF = spark.read.format('xml').option('rowTag', 'record').load('path/to/xml_data')"
  }
  {
    "input_text": "Perform join operations on streaming DataFrames with stateful processing.",
    "output_text": "val joinedStream = stream1.join(stream2, joinCondition).mapGroupsWithState(StateSpec.function(stateUpdateFunction))"
  }
  {
    "input_text": "Perform spatial analysis and geometry operations using Spark Spatial library.",
    "output_text": "val spatialDataAnalysis = df.select(ST_Intersection('geometry1', 'geometry2'))"
  }
  {
    "input_text": "Analyze and process graph data using GraphX library for distributed graph computation.",
    "output_text": "val graph = Graph(vertices, edges)
                    val connectedComponents = graph.connectedComponents().vertices"
  }
                  